# -*- coding: utf-8 -*-
"""Text summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14OL0C8e0GBQkWjjt76rlOYv9Ldtm_Nsv

#  text summarization

Text Summarization do tarah ka hota hai:

Extractive Summarization → Important sentences ko select karta hai.

Abstractive Summarization → Naya summary sentence generate karta hai.

```
 i am build the extractive text summarization
```

This project focuses on text summarization using two approaches:

'''a traditional Seq2Seq model with LSTM and a Transformer-based model. The goal is to see how each model performs and understand the difference between step-by-step sequence processing and attention-based processing'''.

# Steps in the Projects:


```
1. Dataset gathering
2.dataset preprocessing
3.encoding
4.model building (two approach)
5.model evaluation
6.to fetch the live new api
7. to train the model in live new api
8. save the model
9. deploy the model
```

# 1. Dataset gathering

to collect the data in kaggle new_summary.csv

import the basic module
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

summary_dataset_path="/content/drive/MyDrive/dataset/news_summary.csv"
summary_more_dataset_path="/content/drive/MyDrive/dataset/news_summary_more.csv"
summary=pd.read_csv(summary_dataset_path,encoding='iso-8859-1')

summary.head(2)

summary_more=pd.read_csv(summary_more_dataset_path,encoding='iso-8859-1')

summary_more.head(2)

# shape of data
print(summary.shape)
print(summary_more.shape)

print(summary.info())
print(summary_more.info())

summary = summary.iloc[:, 0:6]
summary_more = summary_more.iloc[:, 0:2]

summary['text'] = (
    summary['author'] + ' ' +
    summary['date'] + ' ' +
    summary['read_more'] + ' ' +
    summary['text'] + ' ' +
    summary['ctext']
)

df = pd.DataFrame()

df['text'] = pd.concat([summary_more['text'], summary['text']], ignore_index=True)
df['summary'] = pd.concat([summary_more['headlines'], summary['headlines']], ignore_index=True)

print(df.shape)
df.head(2)

df['text'][0]

print("corresponding summary is :")
df['summary'][0]

print(df['text'][2],"---",df['summary'][2])

df.shape

df.sample(3)

df.head(2)

"""# Data preprocessing

# 1.lower case convert
"""

import re

def clean_text(text):
    """
    - converts to lowercase
    - removes special characters
    - replaces URLs with domain names
    - reduces multiple spaces
    """
    text = str(text).lower()
    text = re.sub(r'https?://([^/\s]+).*', r'\1', text)
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# Tokenization: split text by spaces
def tokenize_texts(texts):
    return [' '.join(clean_text(t).split()) for t in texts]

# Drop null values before cleaning and tokenizing
df.dropna(inplace=True)

processed_text = tokenize_texts(df['text'])
processed_summary = ['_START_ ' + s + ' _END_' for s in tokenize_texts(df['summary'])]

df['cleaned_text'] = pd.Series(processed_text)
df['cleaned_summary'] = pd.Series(processed_summary)

df.head(2)

"""clean_text(t) → text cleaning function (ye lowercasing, punctuation removal, etc. karega).

.split() → text ko spaces ke hisab se words me todta hai.

' '.join(...) → fir un words ko dobara ek consistent space ke sath jodta hai.
"""

df['cleaned_summary'][0]

"""Ye tumhare summary column ko clean karke special tokens add karta hai:

_START_ → summary ke beginning me

_END_ → summary ke end me

 Purpose of START and END tokens:

Ye decoder LSTM ko batate hain kab sequence start aur kab end hota hai.

Ye training aur prediction dono me helpful hote hain.

Model ko “where to stop generating” ka signal milta hai.
"""

print(f"NaN dropped: {df.isna().sum().sum()}")

df = df.dropna(subset=['cleaned_text'])

print(df.shape)

df.head(2)

print("Max text length:", max([len(t.split()) for t in df['cleaned_text']]))
print("Max summary length:", max([len(s.split()) for s in df['summary']]))

df.head(2)

df.drop(['summary'],inplace=True,axis=1)

df.head(2)

df.drop(['text'],inplace=True,axis=1)

df.head(2)

cnt = 0
for i in df['cleaned_text']:
    if len(i.split()) <= 100:
        cnt = cnt + 1
print(f"Percentage of texts with less than 100 words : {cnt / len(df['cleaned_text'])}")

"""When you build your encoder-decoder model (like LSTM, Bi-LSTM, or T5) for text summarization, you need to decide the maximum input sequence length.

This makes your model training faster and more efficient — because it won’t waste time processing unnecessarily long sequences.

```
to decode the max_len(number of token to feed the model)
```
"""

import numpy as np

text_len = [len(t.split()) for t in df['cleaned_text']]
summary_len = [len(s.split()) for s in df['cleaned_summary']]

# 95th percentile cutoff
max_text_len = int(np.percentile(text_len, 95))
max_summary_len = int(np.percentile(summary_len, 95))

print("Suggested max text length:", max_text_len)
print("Suggested max summary length:", max_summary_len)

"""Reasoning:

max_text_len = 100 → covers most of your input texts, keeps LSTM training fast.

max_summary_len = 50 → allows summaries longer than current maximum (15 words) → gives model freedom to generate longer summaries if needed in the future.

# Tokenization

Tokenization¶
This block prepares the text data for a sequence-to-sequence model:

Split dataset: Separates text and summary into training and validation sets to evaluate model performance on unseen data.

Initialize tokenizers: Converts words into integer indices, which neural networks can process.

Analyze rare words: Computes the percentage of words appearing less than thresh times to identify infrequent words that might add noise.

Limit vocabulary to frequent words: Reduces vocabulary size by ignoring rare words, which improves training efficiency and prevents overfitting.

Convert texts to sequences: Maps each word in the texts to its corresponding integer index.

Pad sequences: Ensures all sequences have the same length, necessary for batch processing in neural networks.

Compute final vocabulary size: Includes the padding token to correctly define the input dimension for the model embedding layer
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

"""Tokenizer for text"""

print(max_text_len)

"""reset the max_text_len"""

max_text_len = 100

print(max_text_len)

text_tokenizer = Tokenizer()
text_tokenizer.fit_on_texts(df['cleaned_text'])
text_seq = text_tokenizer.texts_to_sequences(df['cleaned_text'])
text_seq = pad_sequences(text_seq, maxlen=max_text_len, padding='post')

max_summary_len

"""to reset"""

max_summary_len=50

summary_tokenizer = Tokenizer()
summary_tokenizer.fit_on_texts(df['cleaned_summary'])
summary_seq = summary_tokenizer.texts_to_sequences(df['cleaned_summary'])
summary_seq = pad_sequences(summary_seq, maxlen=max_summary_len, padding='post')

"""adding='post' → adds zeros at the end of sequences

Now sequences are uniform length, ready for LSTM input/output
"""

summary_seq[0]

summary_seq[0].shape

text_seq[0]

text_seq[0].shape

text_vocab_size = len(text_tokenizer.word_index) + 1
summary_vocab_size = len(summary_tokenizer.word_index) + 1

print("Text vocab size:", text_vocab_size)
print("Summary vocab size:", summary_vocab_size)

X_train, X_val, y_train, y_val = train_test_split(
    text_seq, summary_seq, test_size=0.1, random_state=42
)

print("Train shape:", X_train.shape, y_train.shape)
print("Validation shape:", X_val.shape, y_val.shape)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding

latent_dim = 200  # Reduced LSTM hidden units
embedding_dim = 50  # Reduced Word embedding dimension

# ---- Encoder ----
encoder_inputs = Input(shape=(max_text_len,))
enc_emb = Embedding(text_vocab_size, embedding_dim, trainable=True)(encoder_inputs)
encoder_lstm = LSTM(latent_dim, return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)
encoder_states = [state_h, state_c]

# ---- Decoder ----
decoder_inputs = Input(shape=(max_summary_len,))
dec_emb_layer = Embedding(summary_vocab_size, embedding_dim, trainable=True)
dec_emb = dec_emb_layer(decoder_inputs)
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)
decoder_dense = Dense(summary_vocab_size, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# ---- Define Model ----
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
model.summary()

y_train_target = np.zeros_like(y_train)
y_train_target[:, :-1] = y_train[:, 1:]  # shift left
y_train_target[:, -1] = 0

y_val_target = np.zeros_like(y_val)
y_val_target[:, :-1] = y_val[:, 1:]
y_val_target[:, -1] = 0

"""add early stopping"""

from tensorflow.keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss',patience=5,verbose=1) # Increased patience

history = model.fit(
    [X_train, y_train],
    y_train_target,
    epochs=5,
    batch_size=64, # Reduced batch size
    validation_data=([X_val, y_val], y_val_target),
    callbacks=[early_stopping]
)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper right')
plt.show

model.save("summary_model.h5")

"""# Model building step 2

using pretrained model T5 small
"""

! pip install tensorflow

import tensorflow as tf
tf.__version__

!pip install transformers==4.33.2
!pip install datasets==2.14.5 evaluate rouge-score
!pip install fsspec==2025.3.0 gcsfs==2025.3.0 --upgrade

"""# Transformers
A Transformer is a deep learning model architecture, introduced in the paper "Attention is All You Need," that uses a "self-attention" mechanism to process sequential data like text or images. By processing the entire input at once and learning the relevance of different parts of the sequence to each other, it effectively handles long-range dependencies and context, making it a powerful tool for tasks such as machine translation, text generation, and image recognition.

Transformer are built on the encode - decoder arch. where both the encoder and decoder are composed of a series of layer the utilize the sefl attention and feed forward neural network.

Self attention:

self attention is a mechanism that can take a static word  embedding and can generate good contexiual embedding.

Load the model and tokenizer
"""

from transformers import TFT5ForConditionalGeneration,T5Tokenizer
import tensorflow as tf

#  Load PEGASUS model & tokenizer
model_name = "t5-small"  # Best for news summarization
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = TFT5ForConditionalGeneration.from_pretrained(model_name,from_pt=True)

print(model.config)

model.config

df.head(2)

print(df['cleaned_summary'][0])

# drop the start and end text in cleaned_summary column
df['cleaned_summary']=df['cleaned_summary'].apply(lambda x:x.replace('_START_ ',''))
df['cleaned_summary']=df['cleaned_summary'].apply(lambda x:x.replace('_END_',''))

print(df['cleaned_summary'][0])

print(df['cleaned_summary'][10])

"""# Full fine tuning"""



# train test split
from sklearn.model_selection import train_test_split
train_text,val_text,train_summary,val_summary=train_test_split(df['cleaned_text'],df['cleaned_summary'],test_size=0.1,random_state=42)

train_text[0]

# calculate the maxlen and max_len output using the 95 percentile method
import numpy as np

def calculate_max_lengths(train_text, val_text, train_summary, val_summary, percentile=95):
    train_input_lens = [len(t.split()) for t in train_text]
    val_input_lens = [len(t.split()) for t in val_text]
    train_output_lens = [len(s.split()) for s in train_summary]
    val_output_lens = [len(s.split()) for s in val_summary]
    max_input_len = int(max(
        np.percentile(train_input_lens, percentile),
        np.percentile(val_input_lens, percentile)
    ))
    max_output_len = int(max(
        np.percentile(train_output_lens, percentile),
        np.percentile(val_output_lens, percentile)
    ))

    print(f" Max input length ({percentile}th percentile): {max_input_len}")
    print(f" Max output length ({percentile}th percentile): {max_output_len}")

    return max_input_len, max_output_len

max_input_len, max_output_len = calculate_max_lengths(
    train_text, val_text, train_summary, val_summary, percentile=95
)

# -------------------------------
# Step 2: Tokenization function
# -------------------------------
def encode_data(texts, summaries, tokenizer, max_input_len=max_input_len, max_output_len=max_output_len):
    input_encodings = tokenizer(
        ["summarize: " + t for t in texts], # because Add task prefix "summarize:" as T5 expects
        max_length=max_input_len,
        padding="max_length",
        truncation=True,
        return_tensors="tf"
    )

    target_encodings = tokenizer(
        list(summaries),
        max_length=max_output_len,
        padding="max_length",
        truncation=True,
        return_tensors="tf"
    )

    labels = target_encodings["input_ids"]
    decoder_input_ids = labels[:, :-1]  # Shift labels to the right for decoder input
# crete a tensorflow dataset()
    dataset = tf.data.Dataset.from_tensor_slices((
        {
            "input_ids": input_encodings["input_ids"],
            "attention_mask": input_encodings["attention_mask"],
            "decoder_input_ids": decoder_input_ids # Include decoder input IDs
        },
        labels[:, 1:] # Shift labels to the left for target
    ))
    return dataset

# -------------------------------
# Step 3: Create train & validation datasets
# -------------------------------
train_dataset = encode_data(train_text, train_summary, tokenizer)
val_dataset = encode_data(val_text, val_summary, tokenizer)

# -------------------------------
# Step 4: Batch, shuffle, prefetch
# -------------------------------
train_dataset = train_dataset.shuffle(1000).batch(4, drop_remainder=True).prefetch(tf.data.AUTOTUNE)
val_dataset = val_dataset.batch(4, drop_remainder=True).prefetch(tf.data.AUTOTUNE)

print("Datasets ready for training!")

# complie the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

model.summary()

model.fit(train_dataset, epochs=2, validation_data=val_dataset)

"""# partial fine tuning

for partial fine-tuning (also called `parameter-efficient fine-tuning`) is an excellent way to:

 Reduce GPU/VRAM usage

 Train much faster

 Prevent overfitting (especially on small datasets)




 Now we’ll modify it slightly to freeze most of the model layers and train only the top (decoder + output projection) parts.

# What happens in fine-tuning

When you fine-tune a pre-trained model (like T5-small),
you are updating its weights (parameters) using your own dataset.

`But T5 has millions of parameters spread across:`


Encoder layers (understanding input text)

Decoder layers (generating output)

Final linear head (vocabulary projection)

`Training all of them:`

 Takes a long time

 Uses huge GPU memory

 Risks overfitting if your dataset is small

🔹 So what does layer.trainable = False do?

It freezes that layer’s weights.

`That means:`

During training, no gradient is computed for that layer

Its parameters stay fixed — same as in the pre-trained model

It reduces GPU and memory usage drastically

`In other words:`

You’re telling TensorFlow: “Don’t touch these layers — keep their learned knowledge.”

🔹 Then why unfreeze only decoder layers later?

After freezing everything, we selectively unfreeze only a few parts


`That means:`

The encoder (language understanding) stays fixed

The decoder (summary generation logic) learns from your dataset

This is the essence of partial fine-tuning —
you’re adapting only the part that generates output, not the part that already understands language.

🔹 Benefits
Type	GPU Usage	Training Speed	Accuracy (on small data)


```
# Full fine-tune	   High	  Slow	 Strong

Partial fine-tune	 Low	  Fast	 Good enough for small/medium datasets
```
"""

for layer in model.layers:
    layer.trainable = False

for layer in model.layers:
    if "decoder" in layer.name or "lm_head" in layer.name:
        layer.trainable = True

# Optional — sometimes improves learning slightly
if hasattr(model, "final_logits_bias"):
    model.final_logits_bias.trainable = True

# complie the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

model.summary()

"""only train the decoder layer"""

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=2
)

"""# Inference"""

import re

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'https?://([^/\s]+).*', r'\1', text)
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def process_text_for_summary(text, tokenizer, max_input_len=512):
    text = clean_text(text)
    inputs = tokenizer(
        "summarize: " + text,
        return_tensors="tf",
        max_length=max_input_len,
        truncation=True,
        padding="max_length"
    )
    return inputs

def generate_summary(inputs, model, tokenizer, max_output_len=150):
    summary_ids = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        min_length=10, # Set a minimum length for the summary
        max_length=max_output_len,
        num_beams=4,
        early_stopping=True
    )
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    print("Generated Summary:", summary)

path="/content/drive/MyDrive/dataset"

model.save_pretrained(path + "/fine_tuned_t5_small")

tokenizer.save_pretrained(path + "/fine_tuned_t5_small")

# Sample text for testing
sample_text = "This is a long piece of text that needs to be summarized. It contains various sentences and information that should be condensed into a shorter summary."

# Process the sample text
inputs = process_text_for_summary(sample_text, tokenizer, max_input_len)

# Generate and print the summary
generate_summary(inputs, model, tokenizer, max_output_len)

def generate_summary_tf(text, model, tokenizer, max_input_len=512, max_output_len=150):
    # Clean and encode input
    text = clean_text(text)
    inputs = tokenizer(
        "summarize: " + text,
        return_tensors="tf",
        max_length=max_input_len,
        truncation=True,
        padding="max_length"
    )

    # Force int32 tensors for TensorFlow
    input_ids = tf.cast(inputs["input_ids"], tf.int32)
    attention_mask = tf.cast(inputs["attention_mask"], tf.int32)

    # Generate summary (TensorFlow-safe)
    summary_ids = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_length=max_output_len,
        min_length=10,
        num_beams=6,
        repetition_penalty=2.5,
        length_penalty=1.0,
        early_stopping=True,
    )

    # Decode tokens into text
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    print("\n🧠 Generated Summary:\n", summary if summary.strip() else "[Empty Output]")
    return summary

print(model.trainable_variables[0].numpy().sum())

